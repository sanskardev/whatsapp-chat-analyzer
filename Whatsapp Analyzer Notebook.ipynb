{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbe960f-189c-4944-98fa-d16de6eec924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d641ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning chat\n",
    "import re\n",
    "\n",
    "chat_file = \"\"\n",
    "\n",
    "pattern = re.compile(r\"^(\\d{1,2}/\\d{1,2}/\\d{2}), (\\d{2}:\\d{2}) - ([^:]+): (.+)$\")\n",
    "\n",
    "messages = []\n",
    "\n",
    "with open(chat_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        match = pattern.match(line.strip())\n",
    "        if match:\n",
    "            date, time, sender, message = match.groups()\n",
    "            if message not in ('null',\n",
    "                               'Missed video call',\n",
    "                               'Missed voice call',\n",
    "                               'Missed group video call',\n",
    "                               'Missed group voice call'):\n",
    "                messages.append({\n",
    "                    \"date\": date,\n",
    "                    \"time\": time,\n",
    "                    \"sender\": sender,\n",
    "                    \"message\": message\n",
    "                })\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71eb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "formatted_messages = []\n",
    "for msg in messages:\n",
    "    # convert 24h â†’ 12h (am/pm)\n",
    "    time_24 = datetime.strptime(msg['time'], \"%H:%M\")\n",
    "    time_12 = time_24.strftime(\"%I:%M %p\").lstrip(\"0\")  # remove leading 0\n",
    "    \n",
    "    line = f\"{msg['date']}, {time_12} - {msg['sender']}: {msg['message']}\"\n",
    "    formatted_messages.append(line)\n",
    "\n",
    "cleantext = \"\\n\".join(formatted_messages)\n",
    "\n",
    "with open('cleaned_chat.txt', 'w', encoding=\"utf-8\") as out:\n",
    "    out.write(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b141f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=m[\"message\"],\n",
    "        metadata={\n",
    "            \"date\": m[\"date\"],\n",
    "            \"time\": m[\"time\"],\n",
    "            \"sender\": m[\"sender\"]\n",
    "        }\n",
    "    )\n",
    "    for m in messages\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"faiss_whatsapp_index\")\n",
    "\n",
    "# this retriever is reading the docs from this program\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# try another method where the retriever is taking docs from the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca198111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "retrieval  = RunnableParallel(\n",
    "    {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "full_context_retrieval = RunnableParallel(\n",
    "    {\"context\": lambda _: cleantext, \"input\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d91937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert on answering specific questions about who said what, dates, details in the chat. Use the following context to answer questions.\\n\\n{context}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert on summarizing the entire conversation or giving overviews. Use the following context to answer questions.\\n\\n{context}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = retrieval | qa_prompt | llm\n",
    "\n",
    "summary_chain = full_context_retrieval | summary_prompt | llm\n",
    "\n",
    "route_system = \"\"\"\n",
    "Given a raw text input to a language model, select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "\n",
    "qa_prompt: prompt seeking answer for specific questions about who said what, dates, details in the chat.\n",
    "summary_prompt: prompt seeking summary, overview or analyzing the entire conversation.\n",
    "\n",
    "<< INPUT >>\n",
    "{input}\n",
    "\n",
    "<< OUTPUT >>\n",
    "Return only one word and the value being either \"qa_prompt\" or \"summary_prompt\" depending on which is the most suitable.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", route_system),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router_chain = route_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d838689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "\n",
    "master_chain = RunnableBranch(\n",
    "    (lambda x: \"qa_prompt\" in router_chain.invoke(x).content.lower(), qa_chain),\n",
    "    (lambda x: \"summary_prompt\" in router_chain.invoke(x).content.lower(), summary_chain),\n",
    "    RunnableLambda(lambda x: \"Sorry, I don't know how to handle that.\")  # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = master_chain.invoke(\"give me a summary of the conversation\")\n",
    "print (result.content)\n",
    "print (result.usage_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
